{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import clip\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Загрузка данных из CSV\n",
    "df = pd.read_csv('flickr30k_images/results.csv', sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[' comment'] = df[' comment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Максимальная длина текста\n",
    "max_length = 75\n",
    "\n",
    "# Функция для укорачивания текста до максимальной длины\n",
    "def truncate_text(text):\n",
    "    if len(text) > max_length:\n",
    "        return text[:max_length]\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# Примените функцию к вашей колонке с текстами (например, 'text_column')\n",
    "df[' comment'] = df[' comment'].apply(truncate_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "#image = preprocess(Image.open(\"CLIP.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize(df[' comment']).to(device)\n",
    "batch_size = 8  # Adjust as needed\n",
    "text_features = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(text), batch_size):\n",
    "        batch_text = text[i:i+batch_size]\n",
    "        batch_features = model.encode_text(batch_text)\n",
    "        text_features.append(batch_features)\n",
    "\n",
    "text_features = torch.cat(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "def encode_images_in_folder(image_folder, model, device):\n",
    "    # Create a list to store image features\n",
    "    image_features_list = []\n",
    "    # Iterate through the images in the folder\n",
    "    for filename in os.listdir(image_folder):\n",
    "        if filename.endswith(('.jpg', '.png', '.jpeg')):\n",
    "            # Construct the full path to the image file\n",
    "            image_path = os.path.join(image_folder, filename)\n",
    "            # Preprocess and encode the image\n",
    "            image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                image_features = model.encode_image(image)\n",
    "            image_features_list.append(image_features)\n",
    "    # Concatenate the image features\n",
    "    if image_features_list:\n",
    "        image_features_tensor = torch.cat(image_features_list, dim=0)\n",
    "        return image_features_tensor\n",
    "    else:\n",
    "        print('No valid images found in the folder.')\n",
    "        return None\n",
    "# Usage example\n",
    "image_folder = 'flickr30k_images/flickr30k_images/flickr30k_images'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model, preprocess = clip.load('ViT-B/32', device=device)\n",
    "image_features = encode_images_in_folder(image_folder, model, device)\n",
    "if image_features is not None:\n",
    "    print(f'Encoded features for {len(image_features)} images.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = preprocess(Image.open(\"flickr30k_images/flickr30k_images/8218549120.jpg\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a woman\"]).to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохранение весов text_features в файл text_features.pt\n",
    "torch.save(text_features, 'text_features.pt')\n",
    "\n",
    "# Сохранение весов image_features в файл image_features.pt\n",
    "torch.save(image_features, 'image_features.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
